{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of emails using Shorttext library\n",
    "\n",
    "This notebook will test the different classification methods offered by the Shorttext library.\n",
    "\n",
    "---\n",
    "__Organization:__\n",
    "1. Put the csv file in the right format for shorttext model and split the data between train and test\n",
    "2. Preprocess the text\n",
    "3. Train a LDA model and make classification using topics found\n",
    "4. Use a Word2Vec representation of words and make classification\n",
    "\n",
    "__What is left to be done:__\n",
    "1. Put the csv file in the right format for shorttext model and split the data between train and test<br>\n",
    "-- Change \"Fatou_relabeled\" by the final dataframe\n",
    "\n",
    "2. Preprocess the text<br>\n",
    "-- Try stemming\n",
    "\n",
    "3. Train a LDA model and make classification using topics found<br>\n",
    "3.1. Train a LDA model and make classification using topics found<br>\n",
    "---- Choose the right number of topics (try different values for k and keep the value that gives the highest cross-validation score (http://scikit-learn.org/stable/modules/cross_validation.html)<br>\n",
    "3.3. Classify using Scikit-Learn Classifiers<br>\n",
    "---- Try different SKLearn classifiers (GaussianNB, GradientBoostingClassifier, etc..)<br>\n",
    "---- Optimize parameters for classifiers (for example for RandomForestClassifier, change number of trees)\n",
    "\n",
    "4. Use a Word2Vec representation of words and make classification<br>\n",
    "4.3. Classify using a Convolutional Neural Network\n",
    "---- Optimize parameters for the CNN (number of epochs, size, etc...) => check Shorttext github<br>\n",
    "---- Try a double CNN<br>\n",
    "\n",
    "4.4. Classify using a C-LSTM Neural Network<br>\n",
    "---- Optimize parameters for the C-LSTM (number of epochs, size, etc...) => check Shorttext github<br>\n",
    "\n",
    "- <strong>What can be tested also:</strong> \n",
    "- Try metrics different than accuracy like f1-score, precision, etc.\n",
    "- Make a comparison of all methods\n",
    "- Add useful graphs\n",
    "\n",
    "__Keep in mind:__\n",
    "\n",
    "Unfortunately, it only works with Python 2. You can create a Python 2 environment using conda <br>see here => https://conda.io/docs/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U shorttext\n",
    "# !pip install -U spacy\n",
    "# !spacy download en\n",
    "import pandas as pd\n",
    "import operator\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import shorttext\n",
    "from shorttext.utils import text_preprocessor\n",
    "from shorttext.utils import load_word2vec_model\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#import seaborn as sns\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "def predict(classifier, mail):\n",
    "    #function that takes a message and a shorttext classifer then predict the category associated\n",
    "    probas = classifier.score(mail)\n",
    "    category = max(probas.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    return(category)\n",
    "\n",
    "\n",
    "def create_df_from_dict(dictionary, categories):\n",
    "    #create a dataframe with columns \"Label\" and \"Message\" from the shorttext dictionary\n",
    "    df = pd.DataFrame()\n",
    "    for cat in categories :\n",
    "        class_size = len(dictionary[cat])\n",
    "        labels = pd.Series([cat]*class_size)\n",
    "        messages = pd.Series(dictionary[cat])\n",
    "        tmp = pd.concat([pd.DataFrame(labels),pd.Series(messages)],axis=1)\n",
    "        tmp.columns = [\"Label\", \"Message\"]\n",
    "        df = pd.concat([df,tmp],axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Put the csv file in the right format for shorttext model and split the data between train and test\n",
    "\n",
    "The file has to obey these rules:\n",
    "\n",
    "- there is a heading; and\n",
    "- there are at least two columns: first the labels, and second the short text under the labels (everything being the second column will be neglected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/recombined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we add the catgegories names\n",
    "categories = [\"miscl.\", \"conflicts\", \"attendance\", \"assignments\", \"enrollment\", \"internal\", \"dsp\", \"regrades\"]\n",
    "df[\"Label\"] = df.Category.apply(lambda cat : categories[cat-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Concatenate the body and the subject\n",
    "df[\"Message\"] = df[\"Subject\"] + \" \" + df[\"Body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.fillna(\"\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the data between train and test\n",
    "def stratified_train_test_split(X, y, test_size, seed):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train.shape', (1064,))\n",
      "('X_test.shape', (456,))\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.3\n",
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = stratified_train_test_split(df[\"Message\"], df[\"Label\"], test_size, seed)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes proportions in train set\n",
      "miscl.         0.379699\n",
      "assignments    0.212406\n",
      "conflicts      0.132519\n",
      "enrollment     0.106203\n",
      "dsp            0.069549\n",
      "attendance     0.047932\n",
      "internal       0.033835\n",
      "regrades       0.017857\n",
      "Name: Label, dtype: float64\n",
      "\n",
      "Classes proportions in test set\n",
      "miscl.         0.379386\n",
      "assignments    0.212719\n",
      "conflicts      0.131579\n",
      "enrollment     0.105263\n",
      "dsp            0.070175\n",
      "attendance     0.048246\n",
      "internal       0.032895\n",
      "regrades       0.019737\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes proportions in train set\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\")\n",
    "print(\"Classes proportions in test set\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>assignments</td>\n",
       "      <td>Re: HW2 forgot to attach screenshot of IPython...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>assignments</td>\n",
       "      <td>Re: Self-Grade hw 8 turned in at 12:02 am grad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>dsp</td>\n",
       "      <td>Re: DSP thanks for your email! please let me k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>assignments</td>\n",
       "      <td>Re:  iPython Submission hi jodie,  unfortunate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>assignments</td>\n",
       "      <td>Minutes Late HW hello ms. li,       yesterday,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Label                                            Message\n",
       "710   assignments  Re: HW2 forgot to attach screenshot of IPython...\n",
       "1192  assignments  Re: Self-Grade hw 8 turned in at 12:02 am grad...\n",
       "418           dsp  Re: DSP thanks for your email! please let me k...\n",
       "647   assignments  Re:  iPython Submission hi jodie,  unfortunate...\n",
       "668   assignments  Minutes Late HW hello ms. li,       yesterday,..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final training dataframe\n",
    "train = pd.concat([y_train, X_train],axis=1)\n",
    "train.columns = [\"Label\", \"Message\"]\n",
    "train.to_csv(\"../data/train_set_in_shorttext_format.csv\", index=False)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>enrollment</td>\n",
       "      <td>Re: Graduating Senior: Enrollment from Waitlis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>miscl.</td>\n",
       "      <td>Lab 109 GSI email? hi,  i am an eecs 47d stude...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>attendance</td>\n",
       "      <td>Re: Lab excuse hi sarah,  we have buffer weeks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>assignments</td>\n",
       "      <td>Re: Uploading Homework problem hi youdong,  i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>miscl.</td>\n",
       "      <td>Re: EE  Final Exam , thank you so much this wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Label                                            Message\n",
       "156   enrollment  Re: Graduating Senior: Enrollment from Waitlis...\n",
       "950       miscl.  Lab 109 GSI email? hi,  i am an eecs 47d stude...\n",
       "685   attendance  Re: Lab excuse hi sarah,  we have buffer weeks...\n",
       "788  assignments  Re: Uploading Homework problem hi youdong,  i ...\n",
       "484       miscl.  Re: EE  Final Exam , thank you so much this wo..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final test dataframe\n",
    "test = pd.concat([y_test, X_test],axis=1)\n",
    "test.columns = [\"Label\", \"Message\"]\n",
    "test.to_csv(\"../data/test_set_in_shorttext_format.csv\", index=False)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess the text\n",
    "\n",
    "- remove punctuation\n",
    "- lemmatize words\n",
    "- put to lower cases\n",
    "- remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dictionary where key = \"category\" and value = list of emails in that category\n",
    "trainclassdict = shorttext.data.retrieve_csvdata_as_dict('../data/train_set_in_shorttext_format.csv')\n",
    "testclassdict = shorttext.data.retrieve_csvdata_as_dict('../data/test_set_in_shorttext_format.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#preprocessing functions\n",
    "step1fcn = lambda s: re.sub(\"[^a-zA-Z]\", \" \", s)\n",
    "step2fcn = lambda s: ' '.join(map(lambda word: lemmatizer.lemmatize(word), s.split(' ')))\n",
    "step3fcn = lambda s: s.lower()\n",
    "step4fcn = lambda s: re.sub(' +',' ',\" \".join([word for word in s.split(\" \") if not word in eng_stopwords]))\n",
    "\n",
    "#pipeline\n",
    "pipeline = [step1fcn, step2fcn, step3fcn, step4fcn]\n",
    "preprocessor = text_preprocessor(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u' maryland blue crab annapolis dog '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"  Maryland blue had crab in, having Annapolis dogs!\"\n",
    "preprocessor(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of cleaning\n",
    "#cat = \"conflicts\"\n",
    "#print(\"Before : {}\".format(trainclassdict[cat][0]))\n",
    "#print(\"\")\n",
    "#print(\"After: {}\".format(preprocessor(trainclassdict[cat][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean the train data\n",
    "for cat in categories :\n",
    "    class_size = len(trainclassdict[cat])\n",
    "    for i in range(class_size):\n",
    "        trainclassdict[cat][i] = preprocessor(trainclassdict[cat][i])\n",
    "\n",
    "#clean the test data       \n",
    "for cat in categories :\n",
    "    class_size = len(testclassdict[cat])\n",
    "    for i in range(class_size):\n",
    "        testclassdict[cat][i] = preprocessor(testclassdict[cat][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create dataframe for train and test\n",
    "train = create_df_from_dict(trainclassdict, categories)\n",
    "test = create_df_from_dict(testclassdict, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classify with LDA model \n",
    "\n",
    "- We train a LDA model with k number of topics (k can be determined by cross-validation)\n",
    "- The LDA model converts every text to a vector\n",
    "- The cos classifier compute the cosinus between the vector representing the text and the vector representing the label\n",
    "- The sklearn classifer uses the coefficients of the vector as features\n",
    "\n",
    "__Reference__: http://shorttext.readthedocs.io/en/latest/tutorial_topic.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Train the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "topicmodeler = shorttext.generators.LDAModeler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics = 7\n",
    "topicmodeler.train(trainclassdict, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = 'exam conflict hi based school policy offering additional accommodation option involved club sport conflict exam time staff member time midterm exam may proctored staff member supervising let know would like take accommodation thanks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04721192,  0.99328852,  0.04722582,  0.04722619,  0.04722399,\n",
       "        0.04721523,  0.04721208])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#topic vector representation\n",
    "topicmodeler.retrieve_topicvec(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Classify using cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_classifier = shorttext.classifiers.TopicVectorCosineDistanceClassifier(topicmodeler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assignments': 0.047225755,\n",
       " 'attendance': 0.99328852,\n",
       " 'conflicts': 0.99328852,\n",
       " 'dsp': 0.074214183,\n",
       " 'enrollment': 0.047211919,\n",
       " 'internal': 0.066969566,\n",
       " 'miscl.': 0.047225755,\n",
       " 'regrades': 0.065808013}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions\n",
    "cos_classifier.score(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attendance'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(cos_classifier, example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Accuracy on train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.44905660377358492)\n"
     ]
    }
   ],
   "source": [
    "train_preds = train.Message.apply(lambda x : predict(cos_classifier, x))\n",
    "train_accuracy = sum(train_preds == train.Label)/float(len(train))\n",
    "print(\"Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.40618101545253865)\n"
     ]
    }
   ],
   "source": [
    "test_preds = test.Message.apply(lambda x : predict(cos_classifier, x))\n",
    "test_accuracy = sum(test_preds == test.Label)/float(len(test))\n",
    "print(\"Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Classify using Scikit-Learn Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sklearn_classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = shorttext.classifiers.TopicVectorSkLearnClassifier(topicmodeler, sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = []\n",
    "y = []\n",
    "classlabels = trainclassdict.keys()\n",
    "for classidx, classlabel in zip(range(len(classlabels)), classlabels):\n",
    "    topicvecs = map(topicmodeler.retrieve_topicvec, trainclassdict[classlabel])\n",
    "    if(np.any(np.isnan(topicvecs))):\n",
    "        i, _ = np.where(np.isnan(topicvecs))\n",
    "        print(i)\n",
    "        print(trainclassdict[classlabel][i[0]])\n",
    "        print(topicmodeler.retrieve_topicvec(trainclassdict[classlabel][i[0]]))\n",
    "    X += topicvecs\n",
    "    y += [classidx]*len(topicvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = shorttext.classifiers.TopicVectorSkLearnClassifier(topicmodeler, sklearn_classifier)\n",
    "classifier.train(trainclassdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assignments': 0.0,\n",
       " 'attendance': 0.0,\n",
       " 'conflicts': 0.0,\n",
       " 'dsp': 0.0,\n",
       " 'enrollment': 0.0,\n",
       " 'internal': 0.0,\n",
       " 'miscl.': 1.0,\n",
       " 'regrades': 0.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions\n",
    "classifier.score(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'miscl.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(classifier, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.95188679245283014)\n"
     ]
    }
   ],
   "source": [
    "train_preds = train.Message.apply(lambda x : predict(classifier, x))\n",
    "train_accuracy = sum(train_preds == train.Label)/float(len(train))\n",
    "print(\"Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>miscl.</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>attendance</th>\n",
       "      <th>assignments</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>internal</th>\n",
       "      <th>dsp</th>\n",
       "      <th>regrades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>miscl.</th>\n",
       "      <td>374</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conflicts</th>\n",
       "      <td>7</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendance</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignments</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enrollment</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internal</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dsp</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regrades</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             miscl.  conflicts  attendance  assignments  enrollment  internal  \\\n",
       "miscl.          374          3           0           21           0         0   \n",
       "conflicts         7        134           0            0           0         0   \n",
       "attendance        0          2          49            0           0         0   \n",
       "assignments      11          0           0          215           0         0   \n",
       "enrollment        2          0           0            0         111         0   \n",
       "internal          0          1           0            0           0        35   \n",
       "dsp               0          0           0            0           0         0   \n",
       "regrades          2          0           0            0           0         0   \n",
       "\n",
       "             dsp  regrades  \n",
       "miscl.         2         0  \n",
       "conflicts      0         0  \n",
       "attendance     0         0  \n",
       "assignments    0         0  \n",
       "enrollment     0         0  \n",
       "internal       0         0  \n",
       "dsp           74         0  \n",
       "regrades       0        17  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_confusion_matrix = pd.DataFrame(confusion_matrix(train.Label, train_preds, labels=categories), columns=categories, index=categories)\n",
    "train_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.54605263157894735)\n"
     ]
    }
   ],
   "source": [
    "test_preds = test.Message.apply(lambda x : predict(classifier, x))\n",
    "test_accuracy = sum(test_preds == test.Label)/float(len(test))\n",
    "print(\"Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>miscl.</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>attendance</th>\n",
       "      <th>assignments</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>internal</th>\n",
       "      <th>dsp</th>\n",
       "      <th>regrades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>miscl.</th>\n",
       "      <td>96</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conflicts</th>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendance</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignments</th>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enrollment</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internal</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dsp</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regrades</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             miscl.  conflicts  attendance  assignments  enrollment  internal  \\\n",
       "miscl.           96          9           3           48           6         1   \n",
       "conflicts        19         27           8            4           0         0   \n",
       "attendance        5         10           4            1           0         1   \n",
       "assignments      28          2           0           65           1         0   \n",
       "enrollment       17          3           1            8          17         1   \n",
       "internal          4          0           0            1           0        10   \n",
       "dsp               4          2           1            1           0         0   \n",
       "regrades          3          0           0            0           0         0   \n",
       "\n",
       "             dsp  regrades  \n",
       "miscl.         8         2  \n",
       "conflicts      2         0  \n",
       "attendance     1         0  \n",
       "assignments    1         0  \n",
       "enrollment     0         1  \n",
       "internal       0         0  \n",
       "dsp           24         0  \n",
       "regrades       0         6  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_confusion_matrix = pd.DataFrame(confusion_matrix(test.Label, test_preds, labels=categories), columns=categories, index=categories)\n",
    "test_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classify with Word2Vec model \n",
    "\n",
    "- We load the previously trained Word2Vec model by Google \n",
    "- We try a classifer that represent a text as the sum of vectors of words\n",
    "- The cos classifier compute the cosinus between the vector representing the text and the vector representing the label\n",
    "- The sklearn classifer uses the coefficients of the vector as features\n",
    "\n",
    "__Reference__: http://shorttext.readthedocs.io/en/latest/tutorial_sumvec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Load the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wvmodel = load_word2vec_model('../../GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Classify using shorttext.classifiers.SumEmbeddedVecClassifier\n",
    "This classifier :\n",
    "- represents the text as a vector which is the sum of vectors representing words\n",
    "- compute the cosinus between this vector and the vector of the labels\n",
    "\n",
    "__Reference__: http://shorttext.readthedocs.io/en/latest/tutorial_sumvec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we should look for the file\n",
    "classifier = shorttext.classifiers.SumEmbeddedVecClassifier(wvmodel)   \n",
    "classifier.train(trainclassdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assignments': 0.75667980249759181,\n",
       " 'attendance': 0.74388712807114454,\n",
       " 'conflicts': 0.83576537017207309,\n",
       " 'dsp': 0.79370996192096444,\n",
       " 'enrollment': 0.77134426783243604,\n",
       " 'internal': 0.78528928847073021,\n",
       " 'miscl.': 0.77228307115211425,\n",
       " 'regrades': 0.71426779882606695}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions\n",
    "classifier.score(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conflicts'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(classifier, example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Accuracy on train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.64245283018867927)\n"
     ]
    }
   ],
   "source": [
    "train_preds = train.Message.apply(lambda x : predict(classifier, x))\n",
    "train_accuracy = sum(train_preds == train.Label)/float(len(train))\n",
    "print(\"Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>miscl.</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>attendance</th>\n",
       "      <th>assignments</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>internal</th>\n",
       "      <th>dsp</th>\n",
       "      <th>regrades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>miscl.</th>\n",
       "      <td>159</td>\n",
       "      <td>56</td>\n",
       "      <td>31</td>\n",
       "      <td>54</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conflicts</th>\n",
       "      <td>12</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendance</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignments</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>180</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enrollment</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internal</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dsp</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regrades</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             miscl.  conflicts  attendance  assignments  enrollment  internal  \\\n",
       "miscl.          159         56          31           54          25        27   \n",
       "conflicts        12        124           0            1           1         0   \n",
       "attendance        5          1          41            0           3         0   \n",
       "assignments      23          1           6          180           8         3   \n",
       "enrollment        9          4          11            0          85         3   \n",
       "internal          4          1           1            2           2        24   \n",
       "dsp               5         12           0            5           1         0   \n",
       "regrades          0          0           2            0           0         0   \n",
       "\n",
       "             dsp  regrades  \n",
       "miscl.        25        23  \n",
       "conflicts      3         0  \n",
       "attendance     0         1  \n",
       "assignments    1         4  \n",
       "enrollment     1         0  \n",
       "internal       2         0  \n",
       "dsp           51         0  \n",
       "regrades       0        17  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_confusion_matrix = pd.DataFrame(confusion_matrix(train.Label, train_preds, labels=categories), columns=categories, index=categories)\n",
    "train_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.59823399558498891)\n"
     ]
    }
   ],
   "source": [
    "test_preds = test.Message.apply(lambda x : predict(classifier, x))\n",
    "test_accuracy = sum(test_preds == test.Label)/float(len(test))\n",
    "print(\"Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>miscl.</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>attendance</th>\n",
       "      <th>assignments</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>internal</th>\n",
       "      <th>dsp</th>\n",
       "      <th>regrades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>miscl.</th>\n",
       "      <td>64</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conflicts</th>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendance</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignments</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enrollment</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internal</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dsp</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regrades</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             miscl.  conflicts  attendance  assignments  enrollment  internal  \\\n",
       "miscl.           64         26          13           21          13        16   \n",
       "conflicts         5         47           1            0           1         0   \n",
       "attendance        1          0          17            0           3         1   \n",
       "assignments      11          4           1           74           2         1   \n",
       "enrollment        8          2           7            0          30         1   \n",
       "internal          0          0           0            2           2        10   \n",
       "dsp               2          1           0            3           0         1   \n",
       "regrades          1          0           3            1           0         0   \n",
       "\n",
       "             dsp  regrades  \n",
       "miscl.        10         7  \n",
       "conflicts      5         1  \n",
       "attendance     0         0  \n",
       "assignments    3         1  \n",
       "enrollment     0         0  \n",
       "internal       1         0  \n",
       "dsp           25         0  \n",
       "regrades       0         4  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_confusion_matrix = pd.DataFrame(confusion_matrix(test.Label, test_preds, labels=categories), columns=categories, index=categories)\n",
    "test_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Classify using a Convolutional Neural Network\n",
    "This uses convolutional Neural Network classifier built with keras.\n",
    "\n",
    "__Reference__: http://shorttext.readthedocs.io/en/latest/tutorial_nnlib.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convnet classifier\n",
    "kmodel = shorttext.classifiers.frameworks.CNNWordEmbed(len(trainclassdict.keys()), vecsize=300)\n",
    "#initialize the classifier\n",
    "classifier = shorttext.classifiers.VarNNEmbeddedVecClassifier(wvmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1060/1060 [==============================] - 0s 251us/step - loss: 1.8242\n",
      "Epoch 2/40\n",
      "1060/1060 [==============================] - 0s 114us/step - loss: 1.5696\n",
      "Epoch 3/40\n",
      "1060/1060 [==============================] - 0s 105us/step - loss: 1.4781\n",
      "Epoch 4/40\n",
      "1060/1060 [==============================] - 0s 106us/step - loss: 1.4477\n",
      "Epoch 5/40\n",
      "1060/1060 [==============================] - 0s 109us/step - loss: 1.4432\n",
      "Epoch 6/40\n",
      "1060/1060 [==============================] - 0s 110us/step - loss: 1.4399\n",
      "Epoch 7/40\n",
      "1060/1060 [==============================] - 0s 108us/step - loss: 1.4369\n",
      "Epoch 8/40\n",
      "1060/1060 [==============================] - 0s 118us/step - loss: 1.4350\n",
      "Epoch 9/40\n",
      "1060/1060 [==============================] - 0s 104us/step - loss: 1.4333\n",
      "Epoch 10/40\n",
      "1060/1060 [==============================] - 0s 107us/step - loss: 1.4333\n",
      "Epoch 11/40\n",
      "1060/1060 [==============================] - 0s 113us/step - loss: 1.4333\n",
      "Epoch 12/40\n",
      "1060/1060 [==============================] - 0s 112us/step - loss: 1.4323\n",
      "Epoch 13/40\n",
      "1060/1060 [==============================] - 0s 106us/step - loss: 1.4329\n",
      "Epoch 14/40\n",
      "1060/1060 [==============================] - 0s 109us/step - loss: 1.4332\n",
      "Epoch 15/40\n",
      "1060/1060 [==============================] - 0s 107us/step - loss: 1.4327\n",
      "Epoch 16/40\n",
      "1060/1060 [==============================] - 0s 106us/step - loss: 1.4340\n",
      "Epoch 17/40\n",
      "1060/1060 [==============================] - 0s 107us/step - loss: 1.4326\n",
      "Epoch 18/40\n",
      "1060/1060 [==============================] - 0s 110us/step - loss: 1.4317\n",
      "Epoch 19/40\n",
      "1060/1060 [==============================] - 0s 107us/step - loss: 1.4354\n",
      "Epoch 20/40\n",
      "1060/1060 [==============================] - 0s 110us/step - loss: 1.4324\n",
      "Epoch 21/40\n",
      "1060/1060 [==============================] - 0s 108us/step - loss: 1.4330\n",
      "Epoch 22/40\n",
      "1060/1060 [==============================] - 0s 107us/step - loss: 1.4322\n",
      "Epoch 23/40\n",
      "1060/1060 [==============================] - 0s 106us/step - loss: 1.4339\n",
      "Epoch 24/40\n",
      "1060/1060 [==============================] - 0s 108us/step - loss: 1.4334\n",
      "Epoch 25/40\n",
      "1060/1060 [==============================] - 0s 110us/step - loss: 1.4311\n",
      "Epoch 26/40\n",
      "1060/1060 [==============================] - 0s 111us/step - loss: 1.4332\n",
      "Epoch 27/40\n",
      "1060/1060 [==============================] - 0s 112us/step - loss: 1.4316\n",
      "Epoch 28/40\n",
      "1060/1060 [==============================] - 0s 109us/step - loss: 1.4319\n",
      "Epoch 29/40\n",
      "1060/1060 [==============================] - 0s 103us/step - loss: 1.4318\n",
      "Epoch 30/40\n",
      "1060/1060 [==============================] - 0s 105us/step - loss: 1.4309\n",
      "Epoch 31/40\n",
      "1060/1060 [==============================] - 0s 106us/step - loss: 1.4329\n",
      "Epoch 32/40\n",
      "1060/1060 [==============================] - 0s 113us/step - loss: 1.4326\n",
      "Epoch 33/40\n",
      "1060/1060 [==============================] - 0s 109us/step - loss: 1.4323\n",
      "Epoch 34/40\n",
      "1060/1060 [==============================] - 0s 110us/step - loss: 1.4345\n",
      "Epoch 35/40\n",
      "1060/1060 [==============================] - 0s 114us/step - loss: 1.4321\n",
      "Epoch 36/40\n",
      "1060/1060 [==============================] - 0s 108us/step - loss: 1.4320\n",
      "Epoch 37/40\n",
      "1060/1060 [==============================] - 0s 106us/step - loss: 1.4326\n",
      "Epoch 38/40\n",
      "1060/1060 [==============================] - 0s 105us/step - loss: 1.4315\n",
      "Epoch 39/40\n",
      "1060/1060 [==============================] - 0s 108us/step - loss: 1.4319\n",
      "Epoch 40/40\n",
      "1060/1060 [==============================] - 0s 105us/step - loss: 1.4311\n"
     ]
    }
   ],
   "source": [
    "#train the classifier\n",
    "classifier.train(trainclassdict, kmodel, nb_epoch=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assignments': 0.00058107381,\n",
       " 'attendance': 0.002994641,\n",
       " 'conflicts': 0.80582738,\n",
       " 'dsp': 0.02148545,\n",
       " 'enrollment': 0.078643925,\n",
       " 'internal': 0.001821973,\n",
       " 'miscl.': 0.088633567,\n",
       " 'regrades': 1.1965838e-05}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Accuracy on train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.58490566037735847)\n"
     ]
    }
   ],
   "source": [
    "train_preds = train.Message.apply(lambda x : predict(classifier, x))\n",
    "train_accuracy = sum(train_preds == train.Label)/float(len(train))\n",
    "print(\"Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>miscl.</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>attendance</th>\n",
       "      <th>assignments</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>internal</th>\n",
       "      <th>dsp</th>\n",
       "      <th>regrades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>miscl.</th>\n",
       "      <td>388</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conflicts</th>\n",
       "      <td>77</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendance</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignments</th>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enrollment</th>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internal</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dsp</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regrades</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             miscl.  conflicts  attendance  assignments  enrollment  internal  \\\n",
       "miscl.          388          4           1            4           2         0   \n",
       "conflicts        77         62           0            0           2         0   \n",
       "attendance       44          0           7            0           0         0   \n",
       "assignments     140          0           0           83           3         0   \n",
       "enrollment       73          1           0            1          38         0   \n",
       "internal         31          0           0            0           1         4   \n",
       "dsp              35          3           0            0           0         0   \n",
       "regrades         15          0           0            2           0         0   \n",
       "\n",
       "             dsp  regrades  \n",
       "miscl.         1         0  \n",
       "conflicts      0         0  \n",
       "attendance     0         0  \n",
       "assignments    0         0  \n",
       "enrollment     0         0  \n",
       "internal       0         0  \n",
       "dsp           36         0  \n",
       "regrades       0         2  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_confusion_matrix = pd.DataFrame(confusion_matrix(train.Label, train_preds, labels=categories), columns=categories, index=categories)\n",
    "train_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.48785871964679911)\n"
     ]
    }
   ],
   "source": [
    "test_preds = test.Message.apply(lambda x : predict(classifier, x))\n",
    "test_accuracy = sum(test_preds == test.Label)/float(len(test))\n",
    "print(\"Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>miscl.</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>attendance</th>\n",
       "      <th>assignments</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>internal</th>\n",
       "      <th>dsp</th>\n",
       "      <th>regrades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>miscl.</th>\n",
       "      <td>165</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conflicts</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendance</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignments</th>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enrollment</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internal</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dsp</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regrades</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             miscl.  conflicts  attendance  assignments  enrollment  internal  \\\n",
       "miscl.          165          1           0            2           0         0   \n",
       "conflicts        50         10           0            0           0         0   \n",
       "attendance       21          0           0            0           1         0   \n",
       "assignments      77          0           0           18           2         0   \n",
       "enrollment       37          0           0            0          11         0   \n",
       "internal         14          0           0            1           0         0   \n",
       "dsp              14          0           0            0           1         0   \n",
       "regrades          9          0           0            0           0         0   \n",
       "\n",
       "             dsp  regrades  \n",
       "miscl.         2         0  \n",
       "conflicts      0         0  \n",
       "attendance     0         0  \n",
       "assignments    0         0  \n",
       "enrollment     0         0  \n",
       "internal       0         0  \n",
       "dsp           17         0  \n",
       "regrades       0         0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_confusion_matrix = pd.DataFrame(confusion_matrix(test.Label, test_preds, labels=categories), columns=categories, index=categories)\n",
    "test_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Classify using a C-LSTM Neural Network\n",
    "This uses a C-LSTM Neural Network classifier built with keras.\n",
    "\n",
    "__Reference__: http://shorttext.readthedocs.io/en/latest/tutorial_nnlib.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convnet classifier\n",
    "kmodel = shorttext.classifiers.frameworks.CLSTMWordEmbed(len(trainclassdict.keys()), vecsize=300)\n",
    "#initialize the classifier\n",
    "classifier = shorttext.classifiers.VarNNEmbeddedVecClassifier(wvmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1060/1060 [==============================] - 1s 557us/step - loss: 1.4314\n",
      "Epoch 2/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4319\n",
      "Epoch 3/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4303\n",
      "Epoch 4/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4311\n",
      "Epoch 5/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4303\n",
      "Epoch 6/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4306\n",
      "Epoch 7/1000\n",
      "1060/1060 [==============================] - 1s 548us/step - loss: 1.4305\n",
      "Epoch 8/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4323\n",
      "Epoch 9/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4305\n",
      "Epoch 10/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4311\n",
      "Epoch 11/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4313\n",
      "Epoch 12/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4302\n",
      "Epoch 13/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4313\n",
      "Epoch 14/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4320\n",
      "Epoch 15/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4297\n",
      "Epoch 16/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4311\n",
      "Epoch 17/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4316\n",
      "Epoch 18/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4296\n",
      "Epoch 19/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4315\n",
      "Epoch 20/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4306\n",
      "Epoch 21/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4312\n",
      "Epoch 22/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4306\n",
      "Epoch 23/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4300\n",
      "Epoch 24/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4308\n",
      "Epoch 25/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4302\n",
      "Epoch 26/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4321\n",
      "Epoch 27/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4310\n",
      "Epoch 28/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4305\n",
      "Epoch 29/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4306\n",
      "Epoch 30/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4301\n",
      "Epoch 31/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4300\n",
      "Epoch 32/1000\n",
      "1060/1060 [==============================] - 1s 526us/step - loss: 1.4310\n",
      "Epoch 33/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4306\n",
      "Epoch 34/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4310\n",
      "Epoch 35/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4317\n",
      "Epoch 36/1000\n",
      "1060/1060 [==============================] - 1s 552us/step - loss: 1.4312\n",
      "Epoch 37/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4308\n",
      "Epoch 38/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4311\n",
      "Epoch 39/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4300\n",
      "Epoch 40/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4308\n",
      "Epoch 41/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4307\n",
      "Epoch 42/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4300\n",
      "Epoch 43/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4314\n",
      "Epoch 44/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4311\n",
      "Epoch 45/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4319\n",
      "Epoch 46/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4300\n",
      "Epoch 47/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4308\n",
      "Epoch 48/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4313\n",
      "Epoch 49/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4300 0s - loss:\n",
      "Epoch 50/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4310\n",
      "Epoch 51/1000\n",
      "1060/1060 [==============================] - 1s 551us/step - loss: 1.4301\n",
      "Epoch 52/1000\n",
      "1060/1060 [==============================] - 1s 553us/step - loss: 1.4307\n",
      "Epoch 53/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4302\n",
      "Epoch 54/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4319\n",
      "Epoch 55/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4300 0s - loss\n",
      "Epoch 56/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4302\n",
      "Epoch 57/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4313\n",
      "Epoch 58/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4303\n",
      "Epoch 59/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4310\n",
      "Epoch 60/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4305\n",
      "Epoch 61/1000\n",
      "1060/1060 [==============================] - 1s 546us/step - loss: 1.4302\n",
      "Epoch 62/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4308\n",
      "Epoch 63/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4306\n",
      "Epoch 64/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4308\n",
      "Epoch 65/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4303\n",
      "Epoch 66/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4303\n",
      "Epoch 67/1000\n",
      "1060/1060 [==============================] - 1s 546us/step - loss: 1.4318\n",
      "Epoch 68/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4299\n",
      "Epoch 69/1000\n",
      "1060/1060 [==============================] - 1s 551us/step - loss: 1.4312\n",
      "Epoch 70/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4304\n",
      "Epoch 71/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4320\n",
      "Epoch 72/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4324\n",
      "Epoch 73/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4308\n",
      "Epoch 74/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4305\n",
      "Epoch 75/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4299\n",
      "Epoch 76/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4309\n",
      "Epoch 77/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4311\n",
      "Epoch 78/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4322\n",
      "Epoch 79/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4301\n",
      "Epoch 80/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4307\n",
      "Epoch 81/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4296\n",
      "Epoch 82/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4308\n",
      "Epoch 83/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4313\n",
      "Epoch 84/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4309\n",
      "Epoch 85/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4292\n",
      "Epoch 86/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4308\n",
      "Epoch 87/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4315\n",
      "Epoch 88/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4312\n",
      "Epoch 89/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4310\n",
      "Epoch 90/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4298\n",
      "Epoch 91/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4305\n",
      "Epoch 92/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4300\n",
      "Epoch 93/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4307 0s - loss:\n",
      "Epoch 94/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4297\n",
      "Epoch 95/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4305\n",
      "Epoch 96/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4306\n",
      "Epoch 97/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4307\n",
      "Epoch 98/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4310\n",
      "Epoch 99/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4313\n",
      "Epoch 100/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4315\n",
      "Epoch 101/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4289 0s - loss:\n",
      "Epoch 102/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4310\n",
      "Epoch 103/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4305\n",
      "Epoch 104/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4308\n",
      "Epoch 105/1000\n",
      "1060/1060 [==============================] - 1s 549us/step - loss: 1.4303\n",
      "Epoch 106/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4300\n",
      "Epoch 107/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4314\n",
      "Epoch 108/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4316\n",
      "Epoch 109/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4301 0s - loss: 1.41\n",
      "Epoch 110/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4302\n",
      "Epoch 111/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4305\n",
      "Epoch 112/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4298\n",
      "Epoch 113/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4299\n",
      "Epoch 114/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4308\n",
      "Epoch 115/1000\n",
      "1060/1060 [==============================] - 1s 526us/step - loss: 1.4297\n",
      "Epoch 116/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4309\n",
      "Epoch 117/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4307\n",
      "Epoch 118/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4297\n",
      "Epoch 119/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4327\n",
      "Epoch 120/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4301\n",
      "Epoch 121/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4313\n",
      "Epoch 122/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4320\n",
      "Epoch 123/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4302\n",
      "Epoch 124/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4300\n",
      "Epoch 125/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4311\n",
      "Epoch 126/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4312\n",
      "Epoch 127/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4312\n",
      "Epoch 128/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4303\n",
      "Epoch 129/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4303\n",
      "Epoch 130/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4312\n",
      "Epoch 131/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4296\n",
      "Epoch 132/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4321\n",
      "Epoch 133/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4307\n",
      "Epoch 134/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4305\n",
      "Epoch 135/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4302 0s - los\n",
      "Epoch 136/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4305\n",
      "Epoch 137/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4310\n",
      "Epoch 138/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4309\n",
      "Epoch 139/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4318\n",
      "Epoch 140/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4301\n",
      "Epoch 141/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4308\n",
      "Epoch 142/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4308\n",
      "Epoch 143/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4307\n",
      "Epoch 144/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4300\n",
      "Epoch 145/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4304\n",
      "Epoch 146/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4310\n",
      "Epoch 147/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4317\n",
      "Epoch 148/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4314\n",
      "Epoch 149/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4322\n",
      "Epoch 150/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4298\n",
      "Epoch 151/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4311\n",
      "Epoch 152/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4292\n",
      "Epoch 153/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4311\n",
      "Epoch 154/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4310\n",
      "Epoch 155/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4292\n",
      "Epoch 156/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4310\n",
      "Epoch 157/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4312\n",
      "Epoch 158/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4295\n",
      "Epoch 159/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4290\n",
      "Epoch 160/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4303\n",
      "Epoch 161/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4304\n",
      "Epoch 162/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4311\n",
      "Epoch 163/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4304\n",
      "Epoch 164/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4315\n",
      "Epoch 165/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4306\n",
      "Epoch 166/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4313\n",
      "Epoch 167/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4302\n",
      "Epoch 168/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4301\n",
      "Epoch 169/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4286\n",
      "Epoch 170/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4305\n",
      "Epoch 171/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4307\n",
      "Epoch 172/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4310\n",
      "Epoch 173/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4307\n",
      "Epoch 174/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4308\n",
      "Epoch 175/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4302\n",
      "Epoch 176/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4304\n",
      "Epoch 177/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4299\n",
      "Epoch 178/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4314\n",
      "Epoch 179/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4299\n",
      "Epoch 180/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4312\n",
      "Epoch 181/1000\n",
      "1060/1060 [==============================] - 1s 526us/step - loss: 1.4291\n",
      "Epoch 182/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4314\n",
      "Epoch 183/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4314\n",
      "Epoch 184/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4293\n",
      "Epoch 185/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4314\n",
      "Epoch 186/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4307\n",
      "Epoch 187/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4306\n",
      "Epoch 188/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4294\n",
      "Epoch 189/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4317\n",
      "Epoch 190/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4302\n",
      "Epoch 191/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4310\n",
      "Epoch 192/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4299\n",
      "Epoch 193/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4306\n",
      "Epoch 194/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4304\n",
      "Epoch 195/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4310\n",
      "Epoch 196/1000\n",
      "1060/1060 [==============================] - ETA: 0s - loss: 1.435 - 1s 546us/step - loss: 1.4314\n",
      "Epoch 197/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4307\n",
      "Epoch 198/1000\n",
      "1060/1060 [==============================] - 1s 552us/step - loss: 1.4303\n",
      "Epoch 199/1000\n",
      "1060/1060 [==============================] - 1s 549us/step - loss: 1.4306\n",
      "Epoch 200/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4327\n",
      "Epoch 201/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4299\n",
      "Epoch 202/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4313\n",
      "Epoch 203/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4317\n",
      "Epoch 204/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4316\n",
      "Epoch 205/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4310\n",
      "Epoch 206/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4311\n",
      "Epoch 207/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4302\n",
      "Epoch 208/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4288\n",
      "Epoch 209/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4301 0s - loss\n",
      "Epoch 210/1000\n",
      "1060/1060 [==============================] - 1s 546us/step - loss: 1.4306\n",
      "Epoch 211/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4302\n",
      "Epoch 212/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4297\n",
      "Epoch 213/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4298\n",
      "Epoch 214/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4316\n",
      "Epoch 215/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4301\n",
      "Epoch 216/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4310\n",
      "Epoch 217/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4302\n",
      "Epoch 218/1000\n",
      "1060/1060 [==============================] - 1s 526us/step - loss: 1.4308\n",
      "Epoch 219/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4295\n",
      "Epoch 220/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4302\n",
      "Epoch 221/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4307\n",
      "Epoch 222/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4324\n",
      "Epoch 223/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4301\n",
      "Epoch 224/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4297\n",
      "Epoch 225/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4312\n",
      "Epoch 226/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4299\n",
      "Epoch 227/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4307\n",
      "Epoch 228/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4300\n",
      "Epoch 229/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4297\n",
      "Epoch 230/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4282\n",
      "Epoch 231/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4297\n",
      "Epoch 232/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4300\n",
      "Epoch 233/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4299\n",
      "Epoch 234/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4299\n",
      "Epoch 235/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4291\n",
      "Epoch 236/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4310\n",
      "Epoch 237/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4298\n",
      "Epoch 238/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4301\n",
      "Epoch 239/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4300\n",
      "Epoch 240/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4299\n",
      "Epoch 241/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4315\n",
      "Epoch 242/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4317\n",
      "Epoch 243/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4297\n",
      "Epoch 244/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4303\n",
      "Epoch 245/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4302\n",
      "Epoch 246/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4302\n",
      "Epoch 247/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4307\n",
      "Epoch 248/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4303\n",
      "Epoch 249/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4303\n",
      "Epoch 250/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4299\n",
      "Epoch 251/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4302\n",
      "Epoch 252/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4327\n",
      "Epoch 253/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4297\n",
      "Epoch 254/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4306\n",
      "Epoch 255/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4299\n",
      "Epoch 256/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4299\n",
      "Epoch 257/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4319\n",
      "Epoch 258/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4302\n",
      "Epoch 259/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4292\n",
      "Epoch 260/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4301\n",
      "Epoch 261/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4295\n",
      "Epoch 262/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4325\n",
      "Epoch 263/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4310\n",
      "Epoch 264/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4305\n",
      "Epoch 265/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4304\n",
      "Epoch 266/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4309\n",
      "Epoch 267/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4309\n",
      "Epoch 268/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4295\n",
      "Epoch 269/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4302\n",
      "Epoch 270/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4305\n",
      "Epoch 271/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4304\n",
      "Epoch 272/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4304\n",
      "Epoch 273/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4304\n",
      "Epoch 274/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4297\n",
      "Epoch 275/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4310\n",
      "Epoch 276/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4310\n",
      "Epoch 277/1000\n",
      "1060/1060 [==============================] - 1s 526us/step - loss: 1.4311\n",
      "Epoch 278/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4309\n",
      "Epoch 279/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4302\n",
      "Epoch 280/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4300\n",
      "Epoch 281/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4292\n",
      "Epoch 282/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4305\n",
      "Epoch 283/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4324\n",
      "Epoch 284/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4303\n",
      "Epoch 285/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4308\n",
      "Epoch 286/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4292\n",
      "Epoch 287/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4308\n",
      "Epoch 288/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4297\n",
      "Epoch 289/1000\n",
      "1060/1060 [==============================] - 1s 526us/step - loss: 1.4309\n",
      "Epoch 290/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4302\n",
      "Epoch 291/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4305\n",
      "Epoch 292/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4291\n",
      "Epoch 293/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4310\n",
      "Epoch 294/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4297\n",
      "Epoch 295/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4301\n",
      "Epoch 296/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4304\n",
      "Epoch 297/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4305\n",
      "Epoch 298/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4300\n",
      "Epoch 299/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4302\n",
      "Epoch 300/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4301\n",
      "Epoch 301/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4304\n",
      "Epoch 302/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4300\n",
      "Epoch 303/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4296\n",
      "Epoch 304/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4304\n",
      "Epoch 305/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4306\n",
      "Epoch 306/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4303\n",
      "Epoch 307/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4313\n",
      "Epoch 308/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4302\n",
      "Epoch 309/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4295\n",
      "Epoch 310/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4303\n",
      "Epoch 311/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4302\n",
      "Epoch 312/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4309 0s - loss:\n",
      "Epoch 313/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4295\n",
      "Epoch 314/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4301\n",
      "Epoch 315/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4290\n",
      "Epoch 316/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4308\n",
      "Epoch 317/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4312\n",
      "Epoch 318/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4300\n",
      "Epoch 319/1000\n",
      "1060/1060 [==============================] - 1s 549us/step - loss: 1.4299\n",
      "Epoch 320/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4310\n",
      "Epoch 321/1000\n",
      "1060/1060 [==============================] - 1s 550us/step - loss: 1.4302\n",
      "Epoch 322/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4303\n",
      "Epoch 323/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4294\n",
      "Epoch 324/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4295\n",
      "Epoch 325/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4296\n",
      "Epoch 326/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4313\n",
      "Epoch 327/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4307\n",
      "Epoch 328/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4299\n",
      "Epoch 329/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4305\n",
      "Epoch 330/1000\n",
      "1060/1060 [==============================] - 1s 526us/step - loss: 1.4296\n",
      "Epoch 331/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4289\n",
      "Epoch 332/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4313\n",
      "Epoch 333/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4300\n",
      "Epoch 334/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4315\n",
      "Epoch 335/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4309\n",
      "Epoch 336/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4307\n",
      "Epoch 337/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4317\n",
      "Epoch 338/1000\n",
      "1060/1060 [==============================] - 1s 526us/step - loss: 1.4305\n",
      "Epoch 339/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4300\n",
      "Epoch 340/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4312\n",
      "Epoch 341/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4301\n",
      "Epoch 342/1000\n",
      "1060/1060 [==============================] - 1s 546us/step - loss: 1.4330\n",
      "Epoch 343/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4313\n",
      "Epoch 344/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4311\n",
      "Epoch 345/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4311\n",
      "Epoch 346/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4303\n",
      "Epoch 347/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4302\n",
      "Epoch 348/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4300\n",
      "Epoch 349/1000\n",
      "1060/1060 [==============================] - 1s 546us/step - loss: 1.4309\n",
      "Epoch 350/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4301\n",
      "Epoch 351/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4296\n",
      "Epoch 352/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4297\n",
      "Epoch 353/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4309\n",
      "Epoch 354/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4291\n",
      "Epoch 355/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4312\n",
      "Epoch 356/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4309\n",
      "Epoch 357/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4308\n",
      "Epoch 358/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4304\n",
      "Epoch 359/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4310\n",
      "Epoch 360/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4295\n",
      "Epoch 361/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4287\n",
      "Epoch 362/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4302 0s - loss:\n",
      "Epoch 363/1000\n",
      "1060/1060 [==============================] - 1s 546us/step - loss: 1.4304\n",
      "Epoch 364/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4297\n",
      "Epoch 365/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4300\n",
      "Epoch 366/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 367/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4313\n",
      "Epoch 368/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4304 0s - loss:\n",
      "Epoch 369/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4300\n",
      "Epoch 370/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4293\n",
      "Epoch 371/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4301\n",
      "Epoch 372/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4304\n",
      "Epoch 373/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4294\n",
      "Epoch 374/1000\n",
      "1060/1060 [==============================] - 1s 546us/step - loss: 1.4308\n",
      "Epoch 375/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4320\n",
      "Epoch 376/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4305\n",
      "Epoch 377/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4299\n",
      "Epoch 378/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4322 0s - loss:\n",
      "Epoch 379/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4305\n",
      "Epoch 380/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4320\n",
      "Epoch 381/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4288\n",
      "Epoch 382/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4299\n",
      "Epoch 383/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4294\n",
      "Epoch 384/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4300\n",
      "Epoch 385/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4313\n",
      "Epoch 386/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4308\n",
      "Epoch 387/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4310\n",
      "Epoch 388/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4306\n",
      "Epoch 389/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4310\n",
      "Epoch 390/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4315\n",
      "Epoch 391/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4308 0s - loss: \n",
      "Epoch 392/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4308\n",
      "Epoch 393/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4311\n",
      "Epoch 394/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4296\n",
      "Epoch 395/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4319\n",
      "Epoch 396/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4287\n",
      "Epoch 397/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4302\n",
      "Epoch 398/1000\n",
      "1060/1060 [==============================] - 1s 551us/step - loss: 1.4311\n",
      "Epoch 399/1000\n",
      "1060/1060 [==============================] - 1s 546us/step - loss: 1.4313\n",
      "Epoch 400/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4312\n",
      "Epoch 401/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4308\n",
      "Epoch 402/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4314\n",
      "Epoch 403/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4302\n",
      "Epoch 404/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4314\n",
      "Epoch 405/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4302\n",
      "Epoch 406/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4304\n",
      "Epoch 407/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4307\n",
      "Epoch 408/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4310\n",
      "Epoch 409/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4304 0s - loss\n",
      "Epoch 410/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4309\n",
      "Epoch 411/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4301\n",
      "Epoch 412/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4312\n",
      "Epoch 413/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4294\n",
      "Epoch 414/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4302\n",
      "Epoch 415/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4300\n",
      "Epoch 416/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4305\n",
      "Epoch 417/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4309\n",
      "Epoch 418/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4301\n",
      "Epoch 419/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4293\n",
      "Epoch 420/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4292\n",
      "Epoch 421/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4308 0s - loss:\n",
      "Epoch 422/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4299\n",
      "Epoch 423/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4294\n",
      "Epoch 424/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4327\n",
      "Epoch 425/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4297\n",
      "Epoch 426/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4294\n",
      "Epoch 427/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4292\n",
      "Epoch 428/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4307\n",
      "Epoch 429/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4303\n",
      "Epoch 430/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4304\n",
      "Epoch 431/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4294\n",
      "Epoch 432/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4289\n",
      "Epoch 433/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4298\n",
      "Epoch 434/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4298\n",
      "Epoch 435/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4299\n",
      "Epoch 436/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4310\n",
      "Epoch 437/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4304\n",
      "Epoch 438/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4303 0s - los\n",
      "Epoch 439/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4297\n",
      "Epoch 440/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4297\n",
      "Epoch 441/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4289\n",
      "Epoch 442/1000\n",
      "1060/1060 [==============================] - 1s 546us/step - loss: 1.4292\n",
      "Epoch 443/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4300\n",
      "Epoch 444/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4299\n",
      "Epoch 445/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4303\n",
      "Epoch 446/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4291\n",
      "Epoch 447/1000\n",
      "1060/1060 [==============================] - 1s 549us/step - loss: 1.4290\n",
      "Epoch 448/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4303\n",
      "Epoch 449/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4307\n",
      "Epoch 450/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4304\n",
      "Epoch 451/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4291\n",
      "Epoch 452/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4308\n",
      "Epoch 453/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4300\n",
      "Epoch 454/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4301\n",
      "Epoch 455/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4298\n",
      "Epoch 456/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4307\n",
      "Epoch 457/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4291\n",
      "Epoch 458/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4303\n",
      "Epoch 459/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4305\n",
      "Epoch 460/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4296\n",
      "Epoch 461/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4309\n",
      "Epoch 462/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4294\n",
      "Epoch 463/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4299\n",
      "Epoch 464/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4312\n",
      "Epoch 465/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4307\n",
      "Epoch 466/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4298\n",
      "Epoch 467/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4314\n",
      "Epoch 468/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4313 0s - los\n",
      "Epoch 469/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4297\n",
      "Epoch 470/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4295\n",
      "Epoch 471/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4300\n",
      "Epoch 472/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4293\n",
      "Epoch 473/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4302\n",
      "Epoch 474/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4308\n",
      "Epoch 475/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4295\n",
      "Epoch 476/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4307\n",
      "Epoch 477/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4302\n",
      "Epoch 478/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4300\n",
      "Epoch 479/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4313\n",
      "Epoch 480/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4307\n",
      "Epoch 481/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4305\n",
      "Epoch 482/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4315\n",
      "Epoch 483/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4298\n",
      "Epoch 484/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4297\n",
      "Epoch 485/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4304\n",
      "Epoch 486/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4305\n",
      "Epoch 487/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4296\n",
      "Epoch 488/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4301\n",
      "Epoch 489/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4308\n",
      "Epoch 490/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4313\n",
      "Epoch 491/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4307\n",
      "Epoch 492/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4305\n",
      "Epoch 493/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4295\n",
      "Epoch 494/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4300\n",
      "Epoch 495/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4303\n",
      "Epoch 496/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4306\n",
      "Epoch 497/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4296\n",
      "Epoch 498/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4292\n",
      "Epoch 499/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4292\n",
      "Epoch 500/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4308\n",
      "Epoch 501/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4303\n",
      "Epoch 502/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4297\n",
      "Epoch 503/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4285\n",
      "Epoch 504/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4298\n",
      "Epoch 505/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4299\n",
      "Epoch 506/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4307\n",
      "Epoch 507/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4310\n",
      "Epoch 508/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4315\n",
      "Epoch 509/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4296\n",
      "Epoch 510/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4298\n",
      "Epoch 511/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4298\n",
      "Epoch 512/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4293\n",
      "Epoch 513/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4292\n",
      "Epoch 514/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4303\n",
      "Epoch 515/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4295\n",
      "Epoch 516/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4301\n",
      "Epoch 517/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4307\n",
      "Epoch 518/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4301\n",
      "Epoch 519/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4299\n",
      "Epoch 520/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4305\n",
      "Epoch 521/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4298\n",
      "Epoch 522/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4304\n",
      "Epoch 523/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4319\n",
      "Epoch 524/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4300\n",
      "Epoch 525/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4298\n",
      "Epoch 526/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4303\n",
      "Epoch 527/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4294\n",
      "Epoch 528/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4314\n",
      "Epoch 529/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4322\n",
      "Epoch 530/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4291\n",
      "Epoch 531/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4296\n",
      "Epoch 532/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4300\n",
      "Epoch 533/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4285\n",
      "Epoch 534/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4311\n",
      "Epoch 535/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4299 0s - loss:\n",
      "Epoch 536/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4287\n",
      "Epoch 537/1000\n",
      "1060/1060 [==============================] - 1s 550us/step - loss: 1.4309\n",
      "Epoch 538/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4299\n",
      "Epoch 539/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4296\n",
      "Epoch 540/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4298\n",
      "Epoch 541/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4298\n",
      "Epoch 542/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4316\n",
      "Epoch 543/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4304\n",
      "Epoch 544/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4310\n",
      "Epoch 545/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4300\n",
      "Epoch 546/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4314\n",
      "Epoch 547/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4291\n",
      "Epoch 548/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4305\n",
      "Epoch 549/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 550/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4307\n",
      "Epoch 551/1000\n",
      "1060/1060 [==============================] - 1s 549us/step - loss: 1.4295\n",
      "Epoch 552/1000\n",
      "1060/1060 [==============================] - 1s 548us/step - loss: 1.4312\n",
      "Epoch 553/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4296\n",
      "Epoch 554/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4301\n",
      "Epoch 555/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4306\n",
      "Epoch 556/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4299\n",
      "Epoch 557/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4299 0s - loss:\n",
      "Epoch 558/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4297\n",
      "Epoch 559/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4314\n",
      "Epoch 560/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4311\n",
      "Epoch 561/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4299\n",
      "Epoch 562/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4301\n",
      "Epoch 563/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4307\n",
      "Epoch 564/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4301\n",
      "Epoch 565/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4308\n",
      "Epoch 566/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4296\n",
      "Epoch 567/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4325\n",
      "Epoch 568/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4305\n",
      "Epoch 569/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4311\n",
      "Epoch 570/1000\n",
      "1060/1060 [==============================] - 1s 553us/step - loss: 1.4300\n",
      "Epoch 571/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4304\n",
      "Epoch 572/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4288\n",
      "Epoch 573/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4307\n",
      "Epoch 574/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4306\n",
      "Epoch 575/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4300\n",
      "Epoch 576/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4290\n",
      "Epoch 577/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4287\n",
      "Epoch 578/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4303\n",
      "Epoch 579/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4299\n",
      "Epoch 580/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4304\n",
      "Epoch 581/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4294\n",
      "Epoch 582/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4302\n",
      "Epoch 583/1000\n",
      "1060/1060 [==============================] - ETA: 0s - loss: 1.423 - 1s 533us/step - loss: 1.4301\n",
      "Epoch 584/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4294\n",
      "Epoch 585/1000\n",
      "1060/1060 [==============================] - 1s 546us/step - loss: 1.4292\n",
      "Epoch 586/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4296\n",
      "Epoch 587/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4311 0s - loss:\n",
      "Epoch 588/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4300\n",
      "Epoch 589/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4300\n",
      "Epoch 590/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4307\n",
      "Epoch 591/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4308\n",
      "Epoch 592/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4291\n",
      "Epoch 593/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4292\n",
      "Epoch 594/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4309\n",
      "Epoch 595/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4301\n",
      "Epoch 596/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4308\n",
      "Epoch 597/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4299\n",
      "Epoch 598/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4307\n",
      "Epoch 599/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4301\n",
      "Epoch 600/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4291\n",
      "Epoch 601/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4301\n",
      "Epoch 602/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4302\n",
      "Epoch 603/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4304\n",
      "Epoch 604/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4308\n",
      "Epoch 605/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4287\n",
      "Epoch 606/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4302 0s - loss:\n",
      "Epoch 607/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4292\n",
      "Epoch 608/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4302\n",
      "Epoch 609/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4297\n",
      "Epoch 610/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4302\n",
      "Epoch 611/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4306\n",
      "Epoch 612/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4305\n",
      "Epoch 613/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4305\n",
      "Epoch 614/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4294\n",
      "Epoch 615/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4300\n",
      "Epoch 616/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4304\n",
      "Epoch 617/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4299\n",
      "Epoch 618/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4293\n",
      "Epoch 619/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4285\n",
      "Epoch 620/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4299\n",
      "Epoch 621/1000\n",
      "1060/1060 [==============================] - 1s 549us/step - loss: 1.4285\n",
      "Epoch 622/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4302\n",
      "Epoch 623/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4288\n",
      "Epoch 624/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4298\n",
      "Epoch 625/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4300\n",
      "Epoch 626/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4311\n",
      "Epoch 627/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4301\n",
      "Epoch 628/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4301\n",
      "Epoch 629/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4296\n",
      "Epoch 630/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4314\n",
      "Epoch 631/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4307\n",
      "Epoch 632/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4299\n",
      "Epoch 633/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4297\n",
      "Epoch 634/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4295\n",
      "Epoch 635/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4293\n",
      "Epoch 636/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4286\n",
      "Epoch 637/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4298\n",
      "Epoch 638/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4294\n",
      "Epoch 639/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4298\n",
      "Epoch 640/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4290\n",
      "Epoch 641/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4294\n",
      "Epoch 642/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4303\n",
      "Epoch 643/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4299\n",
      "Epoch 644/1000\n",
      "1060/1060 [==============================] - 1s 526us/step - loss: 1.4295\n",
      "Epoch 645/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4293\n",
      "Epoch 646/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4308\n",
      "Epoch 647/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4300\n",
      "Epoch 648/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4294\n",
      "Epoch 649/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4282\n",
      "Epoch 650/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4305\n",
      "Epoch 651/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4308\n",
      "Epoch 652/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4298\n",
      "Epoch 653/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4293\n",
      "Epoch 654/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4303\n",
      "Epoch 655/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4299\n",
      "Epoch 656/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4298\n",
      "Epoch 657/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4290 0s - loss: 1.\n",
      "Epoch 658/1000\n",
      "1060/1060 [==============================] - 1s 556us/step - loss: 1.4300\n",
      "Epoch 659/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4306\n",
      "Epoch 660/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4297\n",
      "Epoch 661/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4297\n",
      "Epoch 662/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4286\n",
      "Epoch 663/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4301\n",
      "Epoch 664/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4307\n",
      "Epoch 665/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4297\n",
      "Epoch 666/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4296\n",
      "Epoch 667/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4311\n",
      "Epoch 668/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4305\n",
      "Epoch 669/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4312\n",
      "Epoch 670/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4295\n",
      "Epoch 671/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4302\n",
      "Epoch 672/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4301\n",
      "Epoch 673/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4305\n",
      "Epoch 674/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4307\n",
      "Epoch 675/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4300\n",
      "Epoch 676/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4310\n",
      "Epoch 677/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4294\n",
      "Epoch 678/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4305\n",
      "Epoch 679/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4281\n",
      "Epoch 680/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4296\n",
      "Epoch 681/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4320\n",
      "Epoch 682/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4304\n",
      "Epoch 683/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4302\n",
      "Epoch 684/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4310\n",
      "Epoch 685/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4309\n",
      "Epoch 686/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4293\n",
      "Epoch 687/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4301\n",
      "Epoch 688/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4300\n",
      "Epoch 689/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4303\n",
      "Epoch 690/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4297\n",
      "Epoch 691/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4303\n",
      "Epoch 692/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4282\n",
      "Epoch 693/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4298\n",
      "Epoch 694/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4289\n",
      "Epoch 695/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4311\n",
      "Epoch 696/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4304\n",
      "Epoch 697/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4293\n",
      "Epoch 698/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4300\n",
      "Epoch 699/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4293\n",
      "Epoch 700/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4301\n",
      "Epoch 701/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4306\n",
      "Epoch 702/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4298\n",
      "Epoch 703/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4292\n",
      "Epoch 704/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4297\n",
      "Epoch 705/1000\n",
      "1060/1060 [==============================] - 1s 550us/step - loss: 1.4313\n",
      "Epoch 706/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4300\n",
      "Epoch 707/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4294\n",
      "Epoch 708/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4300\n",
      "Epoch 709/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4302\n",
      "Epoch 710/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4288\n",
      "Epoch 711/1000\n",
      "1060/1060 [==============================] - ETA: 0s - loss: 1.438 - 1s 532us/step - loss: 1.4302\n",
      "Epoch 712/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4302\n",
      "Epoch 713/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4302\n",
      "Epoch 714/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4298\n",
      "Epoch 715/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4292\n",
      "Epoch 716/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4295\n",
      "Epoch 717/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4307\n",
      "Epoch 718/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4303\n",
      "Epoch 719/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4309 0s - loss: \n",
      "Epoch 720/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4304\n",
      "Epoch 721/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4307\n",
      "Epoch 722/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4304\n",
      "Epoch 723/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4303\n",
      "Epoch 724/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4298\n",
      "Epoch 725/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4290\n",
      "Epoch 726/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4294\n",
      "Epoch 727/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4298\n",
      "Epoch 728/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4288\n",
      "Epoch 729/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4298 0s - loss:\n",
      "Epoch 730/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4293\n",
      "Epoch 731/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4287\n",
      "Epoch 732/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4310\n",
      "Epoch 733/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4290\n",
      "Epoch 734/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4296\n",
      "Epoch 735/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4302\n",
      "Epoch 736/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4291\n",
      "Epoch 737/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4288\n",
      "Epoch 738/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4307\n",
      "Epoch 739/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4304\n",
      "Epoch 740/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4296\n",
      "Epoch 741/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4295\n",
      "Epoch 742/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4303 0s - loss\n",
      "Epoch 743/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4309\n",
      "Epoch 744/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4292\n",
      "Epoch 745/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4297\n",
      "Epoch 746/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4298\n",
      "Epoch 747/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4298\n",
      "Epoch 748/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4295\n",
      "Epoch 749/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4299\n",
      "Epoch 750/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4301\n",
      "Epoch 751/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4296\n",
      "Epoch 752/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4304\n",
      "Epoch 753/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4287\n",
      "Epoch 754/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4301\n",
      "Epoch 755/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4300\n",
      "Epoch 756/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4291\n",
      "Epoch 757/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4307\n",
      "Epoch 758/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4302\n",
      "Epoch 759/1000\n",
      "1060/1060 [==============================] - 1s 546us/step - loss: 1.4292\n",
      "Epoch 760/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4292\n",
      "Epoch 761/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4302\n",
      "Epoch 762/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4312\n",
      "Epoch 763/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4305\n",
      "Epoch 764/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4288\n",
      "Epoch 765/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4303\n",
      "Epoch 766/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4295\n",
      "Epoch 767/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4301\n",
      "Epoch 768/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4300\n",
      "Epoch 769/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4294\n",
      "Epoch 770/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4310\n",
      "Epoch 771/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4292\n",
      "Epoch 772/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4295\n",
      "Epoch 773/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4295\n",
      "Epoch 774/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4298\n",
      "Epoch 775/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4297\n",
      "Epoch 776/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4287\n",
      "Epoch 777/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4301\n",
      "Epoch 778/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4291\n",
      "Epoch 779/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4298\n",
      "Epoch 780/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4298\n",
      "Epoch 781/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4299\n",
      "Epoch 782/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4296\n",
      "Epoch 783/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4295\n",
      "Epoch 784/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4303\n",
      "Epoch 785/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4299\n",
      "Epoch 786/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4296\n",
      "Epoch 787/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4304\n",
      "Epoch 788/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4299\n",
      "Epoch 789/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4297\n",
      "Epoch 790/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4294\n",
      "Epoch 791/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4292\n",
      "Epoch 792/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4302\n",
      "Epoch 793/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4317 0s - loss:\n",
      "Epoch 794/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4291\n",
      "Epoch 795/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4298\n",
      "Epoch 796/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4298\n",
      "Epoch 797/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4290\n",
      "Epoch 798/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4297\n",
      "Epoch 799/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4288\n",
      "Epoch 800/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4309\n",
      "Epoch 801/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4297\n",
      "Epoch 802/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4293\n",
      "Epoch 803/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4286\n",
      "Epoch 804/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4293\n",
      "Epoch 805/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4304\n",
      "Epoch 806/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4301\n",
      "Epoch 807/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4288\n",
      "Epoch 808/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4305\n",
      "Epoch 809/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4296\n",
      "Epoch 810/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4312\n",
      "Epoch 811/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4293 0s - loss:\n",
      "Epoch 812/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4311\n",
      "Epoch 813/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4282\n",
      "Epoch 814/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4294\n",
      "Epoch 815/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4304\n",
      "Epoch 816/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4300\n",
      "Epoch 817/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4298\n",
      "Epoch 818/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4293\n",
      "Epoch 819/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4304\n",
      "Epoch 820/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4303\n",
      "Epoch 821/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4299\n",
      "Epoch 822/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4314\n",
      "Epoch 823/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4302\n",
      "Epoch 824/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4303\n",
      "Epoch 825/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4295\n",
      "Epoch 826/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4310\n",
      "Epoch 827/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4294\n",
      "Epoch 828/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4303\n",
      "Epoch 829/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4289\n",
      "Epoch 830/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4285\n",
      "Epoch 831/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4302\n",
      "Epoch 832/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4292\n",
      "Epoch 833/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4307 0s - loss:\n",
      "Epoch 834/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4292\n",
      "Epoch 835/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4300\n",
      "Epoch 836/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4306\n",
      "Epoch 837/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4298\n",
      "Epoch 838/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4303\n",
      "Epoch 839/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4293\n",
      "Epoch 840/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4308\n",
      "Epoch 841/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4296\n",
      "Epoch 842/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4290\n",
      "Epoch 843/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4298\n",
      "Epoch 844/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4301 0s - loss:\n",
      "Epoch 845/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4301\n",
      "Epoch 846/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4301\n",
      "Epoch 847/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4308\n",
      "Epoch 848/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4297\n",
      "Epoch 849/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4301 0s - loss\n",
      "Epoch 850/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4295\n",
      "Epoch 851/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4314\n",
      "Epoch 852/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4292\n",
      "Epoch 853/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4318\n",
      "Epoch 854/1000\n",
      "1060/1060 [==============================] - 1s 558us/step - loss: 1.4295\n",
      "Epoch 855/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4296\n",
      "Epoch 856/1000\n",
      "1060/1060 [==============================] - 1s 526us/step - loss: 1.4290\n",
      "Epoch 857/1000\n",
      "1060/1060 [==============================] - 1s 555us/step - loss: 1.4302\n",
      "Epoch 858/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4303\n",
      "Epoch 859/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4295\n",
      "Epoch 860/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4300\n",
      "Epoch 861/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4309\n",
      "Epoch 862/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4303\n",
      "Epoch 863/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4304\n",
      "Epoch 864/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4304\n",
      "Epoch 865/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4290\n",
      "Epoch 866/1000\n",
      "1060/1060 [==============================] - 1s 526us/step - loss: 1.4294\n",
      "Epoch 867/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4295\n",
      "Epoch 868/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4301\n",
      "Epoch 869/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4288\n",
      "Epoch 870/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4299\n",
      "Epoch 871/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4284\n",
      "Epoch 872/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4298\n",
      "Epoch 873/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4298\n",
      "Epoch 874/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4297\n",
      "Epoch 875/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4305\n",
      "Epoch 876/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4302\n",
      "Epoch 877/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4301\n",
      "Epoch 878/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4298\n",
      "Epoch 879/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4292\n",
      "Epoch 880/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4281\n",
      "Epoch 881/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4293\n",
      "Epoch 882/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4302\n",
      "Epoch 883/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4297\n",
      "Epoch 884/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4286\n",
      "Epoch 885/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4293\n",
      "Epoch 886/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4301\n",
      "Epoch 887/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4303\n",
      "Epoch 888/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4303\n",
      "Epoch 889/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4297\n",
      "Epoch 890/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4303\n",
      "Epoch 891/1000\n",
      "1060/1060 [==============================] - ETA: 0s - loss: 1.425 - 1s 533us/step - loss: 1.4303\n",
      "Epoch 892/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4311\n",
      "Epoch 893/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4302\n",
      "Epoch 894/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4294\n",
      "Epoch 895/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4307\n",
      "Epoch 896/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4302\n",
      "Epoch 897/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4303\n",
      "Epoch 898/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4302\n",
      "Epoch 899/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4298\n",
      "Epoch 900/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4305\n",
      "Epoch 901/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4309\n",
      "Epoch 902/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4297\n",
      "Epoch 903/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4312\n",
      "Epoch 904/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4292\n",
      "Epoch 905/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4290\n",
      "Epoch 906/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4305\n",
      "Epoch 907/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4291\n",
      "Epoch 908/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4293\n",
      "Epoch 909/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4289\n",
      "Epoch 910/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4288\n",
      "Epoch 911/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4301\n",
      "Epoch 912/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4309\n",
      "Epoch 913/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4299\n",
      "Epoch 914/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4311\n",
      "Epoch 915/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4309\n",
      "Epoch 916/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4307\n",
      "Epoch 917/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4293\n",
      "Epoch 918/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4293\n",
      "Epoch 919/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4297\n",
      "Epoch 920/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4296\n",
      "Epoch 921/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4307\n",
      "Epoch 922/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4301\n",
      "Epoch 923/1000\n",
      "1060/1060 [==============================] - 1s 560us/step - loss: 1.4307\n",
      "Epoch 924/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4294\n",
      "Epoch 925/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4289\n",
      "Epoch 926/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4296\n",
      "Epoch 927/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4299\n",
      "Epoch 928/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4294\n",
      "Epoch 929/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4302\n",
      "Epoch 930/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4300\n",
      "Epoch 931/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4301\n",
      "Epoch 932/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4305\n",
      "Epoch 933/1000\n",
      "1060/1060 [==============================] - 1s 548us/step - loss: 1.4306\n",
      "Epoch 934/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4295\n",
      "Epoch 935/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4301\n",
      "Epoch 936/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4304\n",
      "Epoch 937/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4300\n",
      "Epoch 938/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4307\n",
      "Epoch 939/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4296\n",
      "Epoch 940/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4306\n",
      "Epoch 941/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4290\n",
      "Epoch 942/1000\n",
      "1060/1060 [==============================] - 1s 545us/step - loss: 1.4293\n",
      "Epoch 943/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4315\n",
      "Epoch 944/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4310\n",
      "Epoch 945/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4296\n",
      "Epoch 946/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4292\n",
      "Epoch 947/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4299\n",
      "Epoch 948/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4299\n",
      "Epoch 949/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4291\n",
      "Epoch 950/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4309\n",
      "Epoch 951/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4300\n",
      "Epoch 952/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4299\n",
      "Epoch 953/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4301\n",
      "Epoch 954/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4302\n",
      "Epoch 955/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4310\n",
      "Epoch 956/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4304\n",
      "Epoch 957/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4290\n",
      "Epoch 958/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4287\n",
      "Epoch 959/1000\n",
      "1060/1060 [==============================] - 1s 541us/step - loss: 1.4289\n",
      "Epoch 960/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4301\n",
      "Epoch 961/1000\n",
      "1060/1060 [==============================] - 1s 530us/step - loss: 1.4303\n",
      "Epoch 962/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4306\n",
      "Epoch 963/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4294\n",
      "Epoch 964/1000\n",
      "1060/1060 [==============================] - 1s 531us/step - loss: 1.4304\n",
      "Epoch 965/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4292\n",
      "Epoch 966/1000\n",
      "1060/1060 [==============================] - 1s 529us/step - loss: 1.4296\n",
      "Epoch 967/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4300\n",
      "Epoch 968/1000\n",
      "1060/1060 [==============================] - 1s 544us/step - loss: 1.4303\n",
      "Epoch 969/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4303\n",
      "Epoch 970/1000\n",
      "1060/1060 [==============================] - 1s 527us/step - loss: 1.4304\n",
      "Epoch 971/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4304\n",
      "Epoch 972/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4295\n",
      "Epoch 973/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4312\n",
      "Epoch 974/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4288\n",
      "Epoch 975/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4288\n",
      "Epoch 976/1000\n",
      "1060/1060 [==============================] - 1s 547us/step - loss: 1.4296\n",
      "Epoch 977/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4300\n",
      "Epoch 978/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4287\n",
      "Epoch 979/1000\n",
      "1060/1060 [==============================] - 1s 535us/step - loss: 1.4303\n",
      "Epoch 980/1000\n",
      "1060/1060 [==============================] - 1s 537us/step - loss: 1.4293\n",
      "Epoch 981/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4299\n",
      "Epoch 982/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4310\n",
      "Epoch 983/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4299\n",
      "Epoch 984/1000\n",
      "1060/1060 [==============================] - 1s 539us/step - loss: 1.4293 0s - loss\n",
      "Epoch 985/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4305\n",
      "Epoch 986/1000\n",
      "1060/1060 [==============================] - 1s 548us/step - loss: 1.4292\n",
      "Epoch 987/1000\n",
      "1060/1060 [==============================] - 1s 528us/step - loss: 1.4298\n",
      "Epoch 988/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4300\n",
      "Epoch 989/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4307\n",
      "Epoch 990/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4294\n",
      "Epoch 991/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4294\n",
      "Epoch 992/1000\n",
      "1060/1060 [==============================] - 1s 534us/step - loss: 1.4293\n",
      "Epoch 993/1000\n",
      "1060/1060 [==============================] - 1s 543us/step - loss: 1.4292\n",
      "Epoch 994/1000\n",
      "1060/1060 [==============================] - 1s 542us/step - loss: 1.4281\n",
      "Epoch 995/1000\n",
      "1060/1060 [==============================] - 1s 540us/step - loss: 1.4290\n",
      "Epoch 996/1000\n",
      "1060/1060 [==============================] - 1s 533us/step - loss: 1.4290\n",
      "Epoch 997/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4305\n",
      "Epoch 998/1000\n",
      "1060/1060 [==============================] - 1s 532us/step - loss: 1.4301\n",
      "Epoch 999/1000\n",
      "1060/1060 [==============================] - 1s 536us/step - loss: 1.4290\n",
      "Epoch 1000/1000\n",
      "1060/1060 [==============================] - 1s 538us/step - loss: 1.4299\n"
     ]
    }
   ],
   "source": [
    "#train the classifier\n",
    "classifier.train(trainclassdict, kmodel, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assignments': 1.2084547e-05,\n",
       " 'attendance': 3.1170312e-05,\n",
       " 'conflicts': 0.99871206,\n",
       " 'dsp': 0.0004601329,\n",
       " 'enrollment': 0.00015805436,\n",
       " 'internal': 5.4291286e-06,\n",
       " 'miscl.': 0.00062045362,\n",
       " 'regrades': 6.463406e-07}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. Accuracy on train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.61792452830188682)\n"
     ]
    }
   ],
   "source": [
    "train_preds = train.Message.apply(lambda x : predict(classifier, x))\n",
    "train_accuracy = sum(train_preds == train.Label)/float(len(train))\n",
    "print(\"Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>miscl.</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>attendance</th>\n",
       "      <th>assignments</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>internal</th>\n",
       "      <th>dsp</th>\n",
       "      <th>regrades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>miscl.</th>\n",
       "      <td>376</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conflicts</th>\n",
       "      <td>63</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendance</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignments</th>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>114</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enrollment</th>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internal</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dsp</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regrades</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             miscl.  conflicts  attendance  assignments  enrollment  internal  \\\n",
       "miscl.          376         11           1            8           3         0   \n",
       "conflicts        63         74           2            0           1         0   \n",
       "attendance       41          0           9            0           1         0   \n",
       "assignments     106          1           1          114           4         0   \n",
       "enrollment       72          3           0            1          37         0   \n",
       "internal         29          0           0            1           2         4   \n",
       "dsp              30          5           0            0           0         0   \n",
       "regrades         15          0           0            2           0         0   \n",
       "\n",
       "             dsp  regrades  \n",
       "miscl.         1         0  \n",
       "conflicts      0         1  \n",
       "attendance     0         0  \n",
       "assignments    0         0  \n",
       "enrollment     0         0  \n",
       "internal       0         0  \n",
       "dsp           39         0  \n",
       "regrades       0         2  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_confusion_matrix = pd.DataFrame(confusion_matrix(train.Label, train_preds, labels=categories), columns=categories, index=categories)\n",
    "train_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.51876379690949226)\n"
     ]
    }
   ],
   "source": [
    "test_preds = test.Message.apply(lambda x : predict(classifier, x))\n",
    "test_accuracy = sum(test_preds == test.Label)/float(len(test))\n",
    "print(\"Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>miscl.</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>attendance</th>\n",
       "      <th>assignments</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>internal</th>\n",
       "      <th>dsp</th>\n",
       "      <th>regrades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>miscl.</th>\n",
       "      <td>156</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conflicts</th>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendance</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignments</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enrollment</th>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internal</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dsp</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regrades</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             miscl.  conflicts  attendance  assignments  enrollment  internal  \\\n",
       "miscl.          156          6           0            3           2         0   \n",
       "conflicts        40         20           0            0           0         0   \n",
       "attendance       18          2           1            0           1         0   \n",
       "assignments      64          1           1           27           3         0   \n",
       "enrollment       33          2           1            1          11         0   \n",
       "internal         13          0           0            2           0         0   \n",
       "dsp              12          0           0            0           0         0   \n",
       "regrades          9          0           0            0           0         0   \n",
       "\n",
       "             dsp  regrades  \n",
       "miscl.         3         0  \n",
       "conflicts      0         0  \n",
       "attendance     0         0  \n",
       "assignments    1         0  \n",
       "enrollment     0         0  \n",
       "internal       0         0  \n",
       "dsp           20         0  \n",
       "regrades       0         0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_confusion_matrix = pd.DataFrame(confusion_matrix(test.Label, test_preds, labels=categories), columns=categories, index=categories)\n",
    "test_confusion_matrix\n",
    "#horizontal is preds, vertical is actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wvmodel = load_word2vec_model('../../GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from keras.preprocessing import sequence\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed\n",
    "import keras\n",
    "from keras.layers import LSTM, Bidirectional, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "def clean_txt(data):\n",
    "    return [nltk.word_tokenize(x.decode('utf-8').strip()) for x in data]\n",
    "def get_mappings(data, labels):\n",
    "    word_counts = Counter(itertools.chain(*data))\n",
    "    vocabulary_inv = [''] + [x for x, ct in word_counts.items() if ct > 1 or x in wvmodel.vocab] + ['unk']\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "\n",
    "    labs = list(set(labels.as_matrix()))\n",
    "    labmap = {x:i for i, x in enumerate(labs)}\n",
    "    invlabmap = {i:x for i, x in enumerate(labs)}\n",
    "    return vocabulary, vocabulary_inv, labmap, invlabmap\n",
    "def convert_data(data, labels, vocabulary, labmap, max_length=500):\n",
    "    def oh(L):\n",
    "        ar = [0] * len(labmap)\n",
    "        ar[L] = 1\n",
    "        return ar\n",
    "    return sequence.pad_sequences([[vocabulary[word]  if word in vocabulary else vocabulary['unk'] for word in sent] \n",
    "            for sent in data], maxlen=max_length), \\\n",
    "            np.array([oh(labmap[L]) for L in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_train = clean_txt(X_train)\n",
    "cleaned_test = clean_txt(X_test)\n",
    "max_length = 500\n",
    "if train:\n",
    "    vocabulary, vocabulary_inv, labmap, invlabmap = get_mappings(cleaned_train + cleaned_test, \n",
    "                                                                 y_train)\n",
    "else:\n",
    "    import pickle\n",
    "    with open('vocab.pkl', 'rb') as pfile:\n",
    "        vocabulary, vocabulary_inv, labmap, invlabmap = pickle.load(pfile)\n",
    "\n",
    "X_conv, y_conv = convert_data(cleaned_train, y_train, vocabulary, labmap, max_length)\n",
    "X_conv_val, y_conv_val = convert_data(cleaned_test, y_test, vocabulary, labmap, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('vocab.pkl', 'wb') as pfile:\n",
    "#     pickle.dump((vocabulary, vocabulary_inv, labmap, invlabmap ), pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4772, 300)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vocabulary_inv))\n",
    "emb_weights = np.array([wvmodel[w] if w in wvmodel else np.random.uniform(-0.25,0.25,wvmodel.vector_size)\n",
    "               for w in vocabulary_inv])\n",
    "emb_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (None, 500, 300)          1431600   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 100)               140400    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 8)                 808       \n",
      "=================================================================\n",
      "Total params: 1,572,808\n",
      "Trainable params: 1,572,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary_inv), 300, \n",
    "                    input_length=max_length, weights=[emb_weights]))\n",
    "model.add(Bidirectional(LSTM(50)))#, return_sequences=True)))\n",
    "# model.add(LSTM(50))#Bidirectional(\n",
    "# model.add(TimeDistributed(Dense(200)))\n",
    "# model.add(Lambda(function=lambda x: keras.backend.mean(x, axis=1), \n",
    "#                    output_shape=lambda shape: (shape[0],) + shape[2:]))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "optimizer = Adam(lr=2e-4)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1064 samples, validate on 456 samples\n",
      "Epoch 1/1000\n",
      "1064/1064 [==============================] - 22s 21ms/step - loss: 1.9323 - acc: 0.3289 - val_loss: 1.8151 - val_acc: 0.3772\n",
      "Epoch 2/1000\n",
      "1064/1064 [==============================] - 20s 18ms/step - loss: 1.7615 - acc: 0.3788 - val_loss: 1.7196 - val_acc: 0.3794\n",
      "Epoch 3/1000\n",
      "1064/1064 [==============================] - 20s 18ms/step - loss: 1.6984 - acc: 0.3788 - val_loss: 1.6913 - val_acc: 0.3794\n",
      "Epoch 4/1000\n",
      "1064/1064 [==============================] - 20s 19ms/step - loss: 1.6710 - acc: 0.3806 - val_loss: 1.6691 - val_acc: 0.3816\n",
      "Epoch 5/1000\n",
      "1064/1064 [==============================] - 19s 18ms/step - loss: 1.6411 - acc: 0.3882 - val_loss: 1.6348 - val_acc: 0.3969\n",
      "Epoch 6/1000\n",
      "1064/1064 [==============================] - 20s 19ms/step - loss: 1.5876 - acc: 0.4107 - val_loss: 1.5870 - val_acc: 0.4539\n",
      "Epoch 7/1000\n",
      "1064/1064 [==============================] - 20s 18ms/step - loss: 1.5229 - acc: 0.4821 - val_loss: 1.5588 - val_acc: 0.4452\n",
      "Epoch 8/1000\n",
      "1064/1064 [==============================] - 19s 18ms/step - loss: 1.4794 - acc: 0.4878 - val_loss: 1.5297 - val_acc: 0.4583\n",
      "Epoch 9/1000\n",
      "1064/1064 [==============================] - 19s 18ms/step - loss: 1.4289 - acc: 0.5179 - val_loss: 1.4952 - val_acc: 0.4846\n",
      "Epoch 10/1000\n",
      "1064/1064 [==============================] - 19s 18ms/step - loss: 1.3691 - acc: 0.5338 - val_loss: 1.4591 - val_acc: 0.4978\n",
      "Epoch 11/1000\n",
      "1064/1064 [==============================] - 19s 18ms/step - loss: 1.3011 - acc: 0.5545 - val_loss: 1.4197 - val_acc: 0.5219\n",
      "Epoch 12/1000\n",
      "1064/1064 [==============================] - 20s 19ms/step - loss: 1.2247 - acc: 0.5968 - val_loss: 1.3787 - val_acc: 0.5395\n",
      "Epoch 13/1000\n",
      "1064/1064 [==============================] - 19s 18ms/step - loss: 1.1435 - acc: 0.6250 - val_loss: 1.3334 - val_acc: 0.5526\n",
      "Epoch 14/1000\n",
      " 192/1064 [====>.........................] - ETA: 14s - loss: 1.1092 - acc: 0.6354"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    history = model.fit(X_conv, y_conv, nb_epoch=1000, batch_size=64, \n",
    "              validation_data=(X_conv_val, y_conv_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74122807017543857"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if train:\n",
    "    print(np.max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1395684190>]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1395749c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4XaV96KSpzYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13956ebe50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train:\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('bilstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exam conflict hi based school policy offering additional accommodation option involved club sport conflict exam time staff member time midterm exam may proctored staff member supervising let know would like take accommodation thanks'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('bilstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 3s 7ms/step\n",
      "0.714912280702\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_conv_val, y_conv_val)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assignments': 8.4552194e-06,\n",
       " 'attendance': 8.9697755e-07,\n",
       " 'conflicts': 0.99992311,\n",
       " 'dsp': 2.3324557e-05,\n",
       " 'enrollment': 4.7030544e-06,\n",
       " 'internal': 1.4910102e-05,\n",
       " 'miscl.': 2.4378367e-05,\n",
       " 'regrades': 5.9647938e-08}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_single(sentence):\n",
    "    x, _ = convert_data(clean_txt([sentence]), ['attendance'], vocabulary, labmap, max_length)\n",
    "    return {invlabmap[i]:p for i, p in enumerate(model.predict(x)[0])}\n",
    "\n",
    "predict_single(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
